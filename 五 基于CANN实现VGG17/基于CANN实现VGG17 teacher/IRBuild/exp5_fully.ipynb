{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caf577a2-e42d-4a28-b050-6c13d2bd280d",
   "metadata": {},
   "source": [
    "## 基于CANN构建VGG17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d4e39c-8bf8-4e9a-bea2-dbf41f2d4d2a",
   "metadata": {},
   "source": [
    "### 1 实验目标\n",
    "- 基于CANN高性能算子库构建VGG17网络\n",
    "- 加载提供的权重（实验四得到的VGG17权重），编译graph得到离线模型，并利用实验三的测试样例对构建的模型进行推理验证，得到推理结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab249d4-3aba-4008-99f3-27b398cc3dca",
   "metadata": {},
   "source": [
    "### 2 依赖环境\n",
    "- 操作系统：Ubuntu x86\n",
    "- 编译器：g++\n",
    "- 芯片：Ascend310\n",
    "- python及依赖的库：python3.7.5\n",
    "- 已完成昇腾AI软件栈在开发环境上的部署（CANN环境，需要完成驱动及CANN软件的安装）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22211e35-916c-4487-864c-b8cd8af139fd",
   "metadata": {},
   "source": [
    "### 3 构建算子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02a86c-9ac7-4472-96a8-284821b825a1",
   "metadata": {},
   "source": [
    "#### Conv2D  \n",
    "\n",
    "REG_OP(Conv2D) \\\n",
    ".INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8})) //定义数据输入 \\\n",
    ".INPUT(filter, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8})) //定义卷积核输入 \\\n",
    ".OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32})) //可选输入，卷积核的偏置bias \\\n",
    ".OPTIONAL_INPUT(offset_w, TensorType({DT_INT8})) //可选输入，卷积核的offset_w， 仍在算子清单中保留 \\\n",
    ".OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32})) \\\n",
    ".REQUIRED_ATTR(strides, ListInt) //定义Conv2D的属性strides \\\n",
    ".REQUIRED_ATTR(pads, ListInt) //定义Conv2D的属性pads \\\n",
    ".ATTR(dilations, ListInt, {1, 1, 1, 1}) //定义Conv2D的属性dilations \\\n",
    ".ATTR(groups, Int, 1) //定义Conv2D属性group \\\n",
    ".ATTR(data_format, String, \"NHWC\") //定义Conv2D的数据输入格式 \\\n",
    ".ATTR(offset_x, Int, 0) //定义Conv2D的 \\\n",
    ".OP_END_FACTORY_REG(Conv2D) //结束算子注册"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b92e3-0d37-4a4e-b891-38b0f6de1523",
   "metadata": {},
   "source": [
    "从Conv2D算子原型定义可以看到，Conv2D算子包括：\n",
    "- 两个必选输入（INPUT）：x 和 filter\n",
    "- 两个可选输入（OPTIONAL_INPUT）：bias 和 offset_w\n",
    "- 两个必选属性（ATTR）：strides、pads\n",
    "- 四个可选属性（REQUIRED_ATTR）：dilations、groups、data_format、offset_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fbd9ab-5218-43d2-8906-82efcb2d26e6",
   "metadata": {},
   "source": [
    "\n",
    "Operator GenConv2dOp(Shape weight_shape,string conv_name,Operator data){\n",
    "\n",
    "    //构造权重算子的描述信息desc_weight\n",
    "    TensorDesc desc_weight(weight_shape,  FORMAT_NCHW, DT_FLOAT);\n",
    "    //构造tensor \n",
    "    Tensor weight_tensor(desc_weight);\n",
    "    //计算出tensor需要的大小\n",
    "    uint32_t weight_len = weight_shape.GetShapeSize() * sizeof(float);\n",
    "    //从bin文件中加载数据，赋给tensor\n",
    "    bool res = GetConstTensorFromBin(kPath+conv_name+\".weight.bin\", weight_tensor, weight_len);\n",
    "    if (!res) {\n",
    "        cout << __LINE__ << \"GetConstTensorFromBin Failed!\" << endl;\n",
    "    }\n",
    "    //创建Const类型的权重算子，通过Const算子的属性value，传入tensor\n",
    "    auto conv_weight = op::Const(conv_name+\"_weight\")\n",
    "        .set_attr_value(weight_tensor);\n",
    "\n",
    "    //创建卷积算子\n",
    "    auto conv2d = op::Conv2D(conv_name)\n",
    "        .set_input_x(data)    //定义输入，传入上一个算子\n",
    "        .set_input_filter(conv_weight)    //定义卷积核，传入卷积核的权重\n",
    "        .set_attr_strides({ 1, 1, 1, 1 })  //定义strides\n",
    "        .set_attr_pads({ 1, 1, 1, 1 })   //定义pads\n",
    "        .set_attr_dilations({ 1, 1, 1, 1 })  //定义dilations\n",
    "        .set_attr_data_format(\"NCHW\");  //定义输入数据的格式\n",
    "\n",
    "    TensorDesc conv2d_input_desc_x(ge::Shape(), FORMAT_NCHW, DT_FLOAT);\n",
    "    TensorDesc conv2d_input_desc_filter(ge::Shape(), FORMAT_NCHW, DT_FLOAT);\n",
    "    TensorDesc conv2d_output_desc_y(ge::Shape(), FORMAT_NCHW, DT_FLOAT);\n",
    "    conv2d.update_input_desc_x(conv2d_input_desc_x);     //更新卷积的输入信息\n",
    "    conv2d.update_input_desc_filter(conv2d_input_desc_filter);  //更新卷积的filter信息\n",
    "    conv2d.update_output_desc_y(conv2d_output_desc_y);    //更新卷积的输出信息\n",
    "\n",
    "    return conv2d;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb72284-04ea-4b76-b2df-106ea37f1625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ae14057-bd14-4b45-a068-907c923c1a07",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "REG_OP(Data) \\\n",
    ".INPUT(x, TensorType::ALL()) //定义输入信息，输入支持所有数据类型的输入 \\\n",
    ".OUTPUT(y, TensorType::ALL()) //定义输出信息，输出支持所有数据类型的输出 \\\n",
    ".ATTR(index, Int, 0) //定义算子属性 \\\n",
    ".OP_END_FACTORY_REG(Data) //结束算子注册"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefceb1f-3f42-41e1-ab6e-6b1ebe9972b6",
   "metadata": {},
   "source": [
    "代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb8f840-3a0f-4f3f-be31-237f47ad2f38",
   "metadata": {},
   "source": [
    "    auto shape_data = vector<int64_t>({1,3,224,224});//输入数据[N,C,W,H],推理时batchsize为1\n",
    "    TensorDesc desc_data(ge::Shape(shape_data), FORMAT_NCHW, DT_FLOAT); //定义算子信息描述，shape传入desc_data\n",
    "\n",
    "    // 实例化Data算子，名为data\n",
    "    auto data = op::Data(\"data\");\n",
    "    data.update_input_desc_x(desc_data); //更新data1算子的输入数据信息描述，定义输入数据的shape，format和dtype\n",
    "    data.update_output_desc_y(desc_data); //更新data1算子的输出数据信息描述，定义输入数据的shape，format和dtype\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300fe1dd-8172-4d2c-904a-de5da096f895",
   "metadata": {},
   "source": [
    "#### Const"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d5865b-abb0-45d3-a1a8-7c1e7e03b31e",
   "metadata": {},
   "source": [
    "REG_OP(Const) \\\n",
    ".OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE})) //定义算子的输出，支持如上类型 \\\n",
    ".ATTR(value, Tensor, Tensor()) //定义算子的属性，value表示常量算子对应的值 \\\n",
    ".OP_END_FACTORY_REG(Const) //结束算子定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c092a-f2c4-4763-9b0a-d4c30db00005",
   "metadata": {},
   "source": [
    "#### MaxPool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca76c59f-3ad0-4409-bcb3-4e265281073b",
   "metadata": {},
   "source": [
    "REG_OP(MaxPool) \\\n",
    ".INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_QINT8})) \\\n",
    ".OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_QINT8})) \\\n",
    ".REQUIRED_ATTR(ksize, ListInt) \\\n",
    ".REQUIRED_ATTR(strides, ListInt) \\\n",
    ".REQUIRED_ATTR(padding, String) \\\n",
    ".ATTR(data_format, String, \"NHWC\") \\\n",
    ".OP_END_FACTORY_REG(MaxPool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc140ae-e242-4669-81b2-8de503f1f0a1",
   "metadata": {},
   "source": [
    "代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c3047-ed31-4371-b517-1e0d9862c24a",
   "metadata": {},
   "source": [
    "Operator GenMaxpoolOp(string pool_name,Operator data){\n",
    "\n",
    "    auto maxpool = op::MaxPoolV3(pool_name)\n",
    "        .set_input_x(data)\n",
    "        .set_attr_strides({1,1,2,2})  // 代表在四个维度（batch、 height,、width、channels）所移动的步长\n",
    "        .set_attr_ksize({1,1,2,2}) //代表在四个维度（batch、 height,、width、channels）池化的尺寸，一般是[1, height, width, 1]\n",
    "        .set_attr_pads({0,0,0,0})\n",
    "        .set_attr_data_format(\"NCHW\")\n",
    "        .set_attr_padding_mode(\"CALCULATED\")  //padding_mode默认CALCULATED，三种模式 \"SAME\" \"VALID\" or \"CALCULATE\"\n",
    "        .set_attr_global_pooling(false)\n",
    "        .set_attr_ceil_mode(false);  //是否在计算输出shape时，使用向上整取，默认false\n",
    "    \n",
    "    TensorDesc tensor_desc(ge::Shape(), FORMAT_NCHW, DT_FLOAT);\n",
    "    maxpool.update_input_desc_x(tensor_desc);\n",
    "    maxpool.update_output_desc_y(tensor_desc);   \n",
    "    return maxpool;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b16320-8737-4198-94db-34ce00e1b99f",
   "metadata": {},
   "source": [
    "### Relu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23d9e6-3f24-40a8-9184-e81eef46d167",
   "metadata": {},
   "source": [
    "REG_OP(Relu) \\\n",
    ".INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT8, DT_INT32, DT_INT16, DT_INT64, DT_UINT8, DT_UINT16, DT_QINT8})) \\\n",
    ".OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT8, DT_INT32, DT_INT16, DT_INT64, DT_UINT8, DT_UINT16, DT_QINT8})) \\\n",
    ".OP_END_FACTORY_REG(Relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2b405-3f50-4157-8ea9-ca47ecef940d",
   "metadata": {},
   "source": [
    "代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d41400-2c3a-46e4-b209-36f05dcd88b7",
   "metadata": {},
   "source": [
    "Operator GenReluOp(string relu_name,Operator data){\n",
    "\n",
    "\t// 因为relu算子接在bn算子后面，bn算子有多个输出，得指明是data为\"y\"的输出传入relu，防止因BN有多个输出造成图不明确\n",
    "    auto relu = op::Relu(relu_name).set_input_x(data, \"y\");\n",
    "\n",
    "    TensorDesc tensor_desc(ge::Shape(), FORMAT_ND, DT_FLOAT);\n",
    "    relu.update_input_desc_x(tensor_desc);\n",
    "    relu.update_output_desc_y(tensor_desc);\n",
    "    return relu;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ccd19-caac-4889-bd00-39e64f17b169",
   "metadata": {},
   "source": [
    "### BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe1b5ca-891a-449d-8d24-69cb186c789c",
   "metadata": {},
   "source": [
    "REG_OP(BatchNorm) \\\n",
    ".INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT})) \\\n",
    ".INPUT(scale, TensorType({DT_FLOAT})) \\\n",
    ".INPUT(offset, TensorType({DT_FLOAT})) \\\n",
    ".OPTIONAL_INPUT(mean, TensorType({DT_FLOAT})) \\\n",
    ".OPTIONAL_INPUT(variance, TensorType({DT_FLOAT})) \\\n",
    ".OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT})) \\\n",
    ".OUTPUT(batch_mean, TensorType({DT_FLOAT})) \\\n",
    ".OUTPUT(batch_variance, TensorType({DT_FLOAT})) \\\n",
    ".OUTPUT(reserve_space_1, TensorType({DT_FLOAT})) \\\n",
    ".OUTPUT(reserve_space_2, TensorType({DT_FLOAT})) \\\n",
    ".OUTPUT(reserve_space_3, TensorType({DT_FLOAT})) \\\n",
    ".ATTR(epsilon, Float, 0.0001) .ATTR(data_format, String, \"NHWC\") .ATTR(is_training, Bool, true) \\\n",
    ".OP_END_FACTORY_REG(BatchNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085fdcc7-f3f4-4988-9adc-d214a76b5bef",
   "metadata": {},
   "source": [
    "Operator GenBNOp(Shape weight_shape,string bn_name, Operator data){\n",
    "    TensorDesc desc_weight_1(weight_shape, FORMAT_ND, DT_FLOAT);\n",
    "    //定义BN算子的四个权重Const算子，分别对应为BN的offset，scale，mean和variance\n",
    "    Tensor offset_weight_tensor(desc_weight_1);\n",
    "    Tensor scale_weight_tensor(desc_weight_1);\n",
    "    Tensor mean_weight_tensor(desc_weight_1);\n",
    "    Tensor variance_weight_tensor(desc_weight_1);\n",
    "\n",
    "    uint32_t weight_1_len = weight_shape.GetShapeSize() * sizeof(float);\n",
    "    //从bin文件中加载BN的offset，offset对应权重文件中的beta，表示输入偏置项\n",
    "    bool res = GetConstTensorFromBin(kPath+bn_name+\".beta.bin\", offset_weight_tensor, weight_1_len);\n",
    "    if (!res) {\n",
    "        cout << __LINE__ << \"GetConstTensorFromBin Failed!\" << endl;\n",
    "    }\n",
    "    //从bin文件中加载BN的scale，scale对应权重文件中的gamma，表示输入Scalar\n",
    "    res = GetConstTensorFromBin(kPath+bn_name+\".gamma.bin\", scale_weight_tensor, weight_1_len);\n",
    "    if (!res) {\n",
    "        cout << __LINE__ << \"GetConstTensorFromBin Failed!\" << endl;\n",
    "    }\n",
    "     //从bin文件中加载BN的moving_mean，表输入的均值   \n",
    "    res = GetConstTensorFromBin(kPath+bn_name+\".moving_mean.bin\", mean_weight_tensor, weight_1_len);\n",
    "    if (!res) {\n",
    "        cout << __LINE__ << \"GetConstTensorFromBin Failed!\" << endl;\n",
    "    }\n",
    "    //从bin文件中加载BN的moving_variance，表输入的方差\n",
    "    res = GetConstTensorFromBin(kPath+bn_name+\".moving_variance.bin\", variance_weight_tensor, weight_1_len);\n",
    "    if (!res) {\n",
    "        cout << __LINE__ << \"GetConstTensorFromBin Failed!\" << endl;\n",
    "    }   \n",
    "\n",
    "    //构造对应的常量算子，用来定义权重\n",
    "    auto bn_offset = op::Const(bn_name+\"_beta\")\n",
    "        .set_attr_value(offset_weight_tensor);\n",
    "    auto bn_scale = op::Const(bn_name+\"_gamma\")\n",
    "        .set_attr_value(scale_weight_tensor);\n",
    "    auto bn_mean = op::Const(bn_name+\"_mean\")\n",
    "        .set_attr_value(mean_weight_tensor);\n",
    "    auto bn_variance = op::Const(bn_name+\"_variance\")\n",
    "        .set_attr_value(variance_weight_tensor);\n",
    "\n",
    "     //构建bn算子\n",
    "    auto batchnorm = op::BatchNorm(bn_name)\n",
    "        .set_input_x(data)\n",
    "        .set_input_offset(bn_offset)\n",
    "        .set_input_scale(bn_scale)  //设置输入Scalar\n",
    "        .set_input_mean(bn_mean)    //设置输入均值\n",
    "        .set_input_variance(bn_variance)    //设置输入方差\n",
    "        .set_attr_data_format(\"NCHW\")    //设置输入数据的格式NCHW\n",
    "        .set_attr_is_training(false);   //此时非训练状态，设置成false\n",
    "\n",
    "    TensorDesc batchnorm_input_desc_x(ge::Shape(), FORMAT_NCHW, DT_FLOAT);\n",
    "    TensorDesc batchnorm_output_desc_y(ge::Shape(), FORMAT_NCHW, DT_FLOAT);\n",
    "    //更新BN的输入信息\n",
    "    batchnorm.update_input_desc_x(batchnorm_input_desc_x);\n",
    "    batchnorm.update_input_desc_scale(batchnorm_input_desc_x);\n",
    "    batchnorm.update_input_desc_offset(batchnorm_input_desc_x);\n",
    "    batchnorm.update_input_desc_mean(batchnorm_input_desc_x);\n",
    "    batchnorm.update_input_desc_variance(batchnorm_input_desc_x);\n",
    "\n",
    "    batchnorm.update_output_desc_y(batchnorm_output_desc_y);\n",
    "    batchnorm.update_output_desc_batch_mean(batchnorm_output_desc_y);\n",
    "    batchnorm.update_output_desc_batch_variance(batchnorm_output_desc_y);\n",
    "\n",
    "    return batchnorm;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340c826-535a-4bc5-8482-93f12ecfaa95",
   "metadata": {},
   "source": [
    "#### Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4cfe5-d95f-409b-81fa-cc5f7d170fd6",
   "metadata": {},
   "source": [
    "REG_OP(Flatten) \\\n",
    ".INPUT(x, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_FLOAT, DT_FLOAT16})) \\\n",
    ".OUTPUT(y, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_FLOAT, DT_FLOAT16})) \\\n",
    ".ATTR(axis, Int, 1) \\\n",
    ".OP_END_FACTORY_REG(Flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad0b1b-1e20-4bc1-8883-8ed2cf376c28",
   "metadata": {},
   "source": [
    "代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7794ae6a-a794-4225-a4b7-0399d03a3072",
   "metadata": {},
   "source": [
    "Operator GenFlattenOp(string flatten_name,Operator data){\n",
    "    //构建Flatten算子\n",
    "    auto flatten = op::FlattenV2(flatten_name).set_input_x(data);\n",
    "    //更新算子输入输出信息\n",
    "    TensorDesc tensor_desc(ge::Shape(), FORMAT_ND, DT_FLOAT);\n",
    "    flatten.update_input_desc_x(tensor_desc);\n",
    "    flatten.update_output_desc_y(tensor_desc); \n",
    "\n",
    "    return flatten;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02e4d0-e47c-47bb-a514-f90e3d7c98b3",
   "metadata": {},
   "source": [
    "#### Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b95bda-c1a4-4846-8840-e35b55bdc829",
   "metadata": {},
   "source": [
    "REG_OP(MatMulV2) \\\n",
    ".INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8, DT_INT4, DT_BF16})) \\\n",
    ".INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8, DT_INT4, DT_BF16})) \\\n",
    ".OPTIONAL_INPUT(bias, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16})) \\\n",
    ".OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16})) \\\n",
    ".OPTIONAL_INPUT(offset_w, TensorType({DT_INT8, DT_INT4})) \\\n",
    ".ATTR(transpose_x1, Bool, false) \\\n",
    ".ATTR(transpose_x2, Bool, false) \\\n",
    ".ATTR(offset_x, Int, 0) \\\n",
    ".OP_END_FACTORY_REG(MatMulV2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532da90-8049-4c04-abfe-4c89b1802d92",
   "metadata": {},
   "source": [
    "代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658518ac-93ee-4fd4-a28c-2ad68ae4f34e",
   "metadata": {},
   "source": [
    "Operator GenDenseOp(uint32_t input_channel,uint32_t output_channel,string dense_name,Operator data){\n",
    "\n",
    "\n",
    "    // 构造dense层的权重矩阵，权重来自bin文件\n",
    "    auto matmul_weight_shape = ge::Shape({output_channel, input_channel});\n",
    "    TensorDesc desc_matmul_weight(matmul_weight_shape, FORMAT_ND, DT_FLOAT);\n",
    "    Tensor matmul_weight_tensor(desc_matmul_weight);\n",
    "    uint32_t matmul_weight_len = matmul_weight_shape.GetShapeSize() * sizeof(float);\n",
    "    bool res = GetConstTensorFromBin(kPath + dense_name+\".weight.bin\", matmul_weight_tensor, matmul_weight_len);\n",
    "    if (!res) {\n",
    "        cout << __LINE__ << \"GetConstTensorFromBin Failed!\" << endl;\n",
    "    }\n",
    "    //构造matmul算子的权重常量算子\n",
    "    auto matmul_weight = op::Const(dense_name+\"_weight\")\n",
    "        .set_attr_value(matmul_weight_tensor);\n",
    "\n",
    "    //构造偏重常量算子，读取偏置参数，作为OPTIONAL_INPUT的bias输入\n",
    "    auto bias_add_shape = ge::Shape({ output_channel });\n",
    "    TensorDesc desc_bias_add_const(bias_add_shape, FORMAT_ND, DT_FLOAT);\n",
    "    Tensor bias_add_const_tensor(desc_bias_add_const);\n",
    "    uint32_t bias_add_const_len = bias_add_shape.GetShapeSize() * sizeof(float);\n",
    "    res = GetConstTensorFromBin(kPath + dense_name+\".bias.bin\", bias_add_const_tensor, bias_add_const_len);\n",
    "    if (!res) {\n",
    "        cout << __LINE__ << \"GetConstTensorFromBin Failed!\" << endl;\n",
    "    }\n",
    "\n",
    "    auto bias_add_const = op::Const(dense_name+\"_bias\")\n",
    "        .set_attr_value(bias_add_const_tensor);\n",
    "    \n",
    "    // 构造MatMulV2算子，三个输入，权重矩阵W，flatten后的输入数据X，偏置bias\n",
    "    auto matmul = op::MatMulV2(dense_name+\"_matmul\")\n",
    "        .set_input_x1(data)\n",
    "        .set_input_x2(matmul_weight)\n",
    "\t.set_attr_transpose_x2(true)\n",
    "        .set_input_bias(bias_add_const);\n",
    "\n",
    "    // 更新算子描述信息\n",
    "    TensorDesc tensor_desc_matmul(ge::Shape(), FORMAT_ND, DT_FLOAT);\n",
    "    matmul.update_input_desc_x1(tensor_desc_matmul);\n",
    "    matmul.update_input_desc_x2(tensor_desc_matmul);\n",
    "    matmul.update_input_desc_bias(tensor_desc_matmul);\n",
    "    matmul.update_output_desc_y(tensor_desc_matmul);\n",
    "\n",
    "    return matmul;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7bb0ca-3f68-4f3d-99e7-4c492609bc87",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07369e27-8065-49d0-af1a-510b9ebe2011",
   "metadata": {},
   "source": [
    "REG_OP(SoftmaxV2) \\\n",
    ".INPUT(x, TensorType({DT_DOUBLE, DT_FLOAT16, DT_FLOAT})) \\\n",
    ".OUTPUT(y, TensorType({DT_DOUBLE, DT_FLOAT16, DT_FLOAT})) \\\n",
    ".ATTR(axes, ListInt, {-1}) \\\n",
    ".OP_END_FACTORY_REG(SoftmaxV2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5797fe-c2fb-48e3-b9fc-8c2eaa8b7511",
   "metadata": {},
   "source": [
    "代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26c719-be8a-48db-928e-5af7d62c4c76",
   "metadata": {},
   "source": [
    "Operator GenSoftmaxOp(string flatten_name, Operator data){\n",
    "\n",
    "    auto softmax = op::SoftmaxV2(flatten_name).set_input_x(data);   //softmax默认axes为-1\n",
    "    //auto softmax = op::Softmax(flatten_name).set_input_x(data);   //softmax默认axes为-1\n",
    "    return softmax;\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43f76ea8-3146-4e1c-9c15-b4da911dc8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p out\n",
      "g++ .//src/main.cpp -I /usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc -I /usr/local/Ascend/ascend-toolkit/latest/atc/include/graph -I /usr/local/Ascend/ascend-toolkit/latest/atc/include/ge -I /usr/local/Ascend/ascend-toolkit/latest/atc/include/parser -I /usr/local/Ascend/ascend-toolkit/latest/atc/include  -L /usr/local/Ascend/ascend-toolkit/latest/atc/lib64/stub -lgraph -lge_compiler -lfmk_parser  -std=c++11 -g -Wall -D_GLIBCXX_USE_CXX11_ABI=0 -o ./out/ir_build \n",
      "\u001b[01m\u001b[K.//src/main.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kge::Operator GenConv2dOp(ge::Shape, std::string, ge::Operator)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K.//src/main.cpp:108:53:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::Const::Const(const string&)\u001b[m\u001b[K’ is deprecated: Please use Const(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto conv_weight = op::Const(conv_name+\"_weight\"\u001b[01;35m\u001b[K)\u001b[m\u001b[K\n",
      "                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/array_ops.h:476:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KC\u001b[m\u001b[Konst)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:112:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::Conv2D::Conv2D(const string&)\u001b[m\u001b[K’ is deprecated: Please use Conv2D(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto conv2d = op::Conv2D(conv_name\u001b[01;35m\u001b[K)\u001b[m\u001b[K\n",
      "                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/nn_calculation_ops.h:836:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KC\u001b[m\u001b[Konv2D)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kge::Operator GenBNOp(ge::Shape, std::string, ge::Operator)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K.//src/main.cpp:167:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::Const::Const(const string&)\u001b[m\u001b[K’ is deprecated: Please use Const(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto bn_offset = op::Const(bn_name+\"_beta\"\u001b[01;35m\u001b[K)\u001b[m\u001b[K\n",
      "                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/array_ops.h:476:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KC\u001b[m\u001b[Konst)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:169:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::Const::Const(const string&)\u001b[m\u001b[K’ is deprecated: Please use Const(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto bn_scale = op::Const(bn_name+\"_gamma\"\u001b[01;35m\u001b[K)\u001b[m\u001b[K\n",
      "                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/array_ops.h:476:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KC\u001b[m\u001b[Konst)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:171:45:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::Const::Const(const string&)\u001b[m\u001b[K’ is deprecated: Please use Const(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto bn_mean = op::Const(bn_name+\"_mean\"\u001b[01;35m\u001b[K)\u001b[m\u001b[K\n",
      "                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/array_ops.h:476:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KC\u001b[m\u001b[Konst)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:173:53:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::Const::Const(const string&)\u001b[m\u001b[K’ is deprecated: Please use Const(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto bn_variance = op::Const(bn_name+\"_variance\"\u001b[01;35m\u001b[K)\u001b[m\u001b[K\n",
      "                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/array_ops.h:476:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KC\u001b[m\u001b[Konst)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:177:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::BatchNorm::BatchNorm(const string&)\u001b[m\u001b[K’ is deprecated: Please use BatchNorm(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto batchnorm = op::BatchNorm(bn_name\u001b[01;35m\u001b[K)\u001b[m\u001b[K\n",
      "                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/nn_batch_norm_ops.h:130:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KB\u001b[m\u001b[KatchNorm)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kge::Operator GenReluOp(std::string, ge::Operator)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K.//src/main.cpp:210:35:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::Relu::Relu(const string&)\u001b[m\u001b[K’ is deprecated: Please use Relu(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto relu = op::Relu(relu_name\u001b[01;35m\u001b[K)\u001b[m\u001b[K.set_input_x(data, \"y\");\n",
      "                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/nonlinear_fuc_ops.h:253:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KR\u001b[m\u001b[Kelu)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:210:58:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::Relu::_THIS_TYPE& ge::op::Relu::set_input_x(ge::Operator&, const string&)\u001b[m\u001b[K’ is deprecated: Please use _THIS_TYPE &set_input_x_by_name(Operator &, const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto relu = op::Relu(relu_name).set_input_x(data, \"y\"\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
      "                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:341:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      "   _THIS_TYPE &\u001b[01;36m\u001b[Ks\u001b[m\u001b[Ket_input_##x(Operator &v, const std::string &srcName) {         \\\n",
      "               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/nonlinear_fuc_ops.h:254:6:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KINPUT\u001b[m\u001b[K’\n",
      "     .\u001b[01;36m\u001b[KINPUT\u001b[m\u001b[K(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE,\n",
      "      \u001b[01;36m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kge::Operator GenMaxpoolOp(std::string, ge::Operator)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K.//src/main.cpp:225:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::MaxPoolV3::MaxPoolV3(const string&)\u001b[m\u001b[K’ is deprecated: Please use MaxPoolV3(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto maxpool = op::MaxPoolV3(pool_name\u001b[01;35m\u001b[K)\u001b[m\u001b[K\n",
      "                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/nn_pooling_ops.h:1468:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KM\u001b[m\u001b[KaxPoolV3)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kge::Operator GenFlattenOp(std::string, ge::Operator)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K.//src/main.cpp:248:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::FlattenV2::FlattenV2(const string&)\u001b[m\u001b[K’ is deprecated: Please use FlattenV2(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto flatten = op::FlattenV2(flatten_name\u001b[01;35m\u001b[K)\u001b[m\u001b[K.set_input_x(data);\n",
      "                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/transformation_ops.h:704:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KF\u001b[m\u001b[KlattenV2)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kge::Operator GenDenseOp(uint32_t, uint32_t, std::string, ge::Operator)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K.//src/main.cpp:277:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::Const::Const(const string&)\u001b[m\u001b[K’ is deprecated: Please use Const(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto matmul_weight = op::Const(dense_name+\"_weight\"\u001b[01;35m\u001b[K)\u001b[m\u001b[K\n",
      "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/array_ops.h:476:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KC\u001b[m\u001b[Konst)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:290:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::Const::Const(const string&)\u001b[m\u001b[K’ is deprecated: Please use Const(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto bias_add_const = op::Const(dense_name+\"_bias\"\u001b[01;35m\u001b[K)\u001b[m\u001b[K\n",
      "                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/array_ops.h:476:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KC\u001b[m\u001b[Konst)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:294:52:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::MatMulV2::MatMulV2(const string&)\u001b[m\u001b[K’ is deprecated: Please use MatMulV2(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto matmul = op::MatMulV2(dense_name+\"_matmul\"\u001b[01;35m\u001b[K)\u001b[m\u001b[K\n",
      "                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/matrix_calculation_ops.h:267:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KM\u001b[m\u001b[KatMulV2)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K.//src/main.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kge::Operator GenSoftmaxOp(std::string, ge::Operator)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K.//src/main.cpp:316:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kge::op::SoftmaxV2::SoftmaxV2(const string&)\u001b[m\u001b[K’ is deprecated: Please use SoftmaxV2(const char *) instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
      "     auto softmax = op::SoftmaxV2(flatten_name\u001b[01;35m\u001b[K)\u001b[m\u001b[K.set_input_x(data);   //softmax默认axes为-1\n",
      "                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/aipp.h:24:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/all_ops.h:24\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K.//src/main.cpp:31\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/opp/op_proto/built-in/inc/nn_norm_ops.h:333:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " REG_OP(\u001b[01;36m\u001b[KS\u001b[m\u001b[KoftmaxV2)\n",
      "        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/usr/local/Ascend/ascend-toolkit/latest/atc/include/graph/operator_reg.h:256:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KREG_OP\u001b[m\u001b[K’\n",
      "     explicit \u001b[01;36m\u001b[Kx\u001b[m\u001b[K(const std::string &name) : Operator(name.c_str(), #x) { __##x(); } \\\n",
      "              \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "collect2: fatal error: cannot find 'ld'\n",
      "compilation terminated.\n",
      "make: *** [Makefile:51: ir_build] Error 1\n",
      "ERROR: compile failed, please check your project\n"
     ]
    }
   ],
   "source": [
    "! scripts/testcase_300.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Sep 13 2022, 22:03:16) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
