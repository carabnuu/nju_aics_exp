{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc2fb5a-d343-4c72-8403-694a4a8e8823",
   "metadata": {},
   "source": [
    "## 实验七：基于MindSpore推理的量化实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017fc9d7-1920-467a-87c7-ef485816feb9",
   "metadata": {},
   "source": [
    "本实验利用先前的VGG16网络实现基于MindSpore框架的量化推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb8d05-65ea-4ef5-8265-ccbcc253337c",
   "metadata": {},
   "source": [
    "### 1. 实验目的\n",
    "- 了解神经网络量化操作，能够独立实现int8量化操作\n",
    "- 构建量化VGG16神经网络\n",
    "- 基于MindSpore框架实现量化推理，能够独立编写量化操作代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d5aec1-cb73-4e62-95bf-32ac8349e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入依赖包\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "from mindspore import ops\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "from mindspore import context\n",
    "context.set_context(mode=context.PYNATIVE_MODE, device_target='CPU')\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c38a5a-7247-4f1e-8a4f-882343cdc0de",
   "metadata": {},
   "source": [
    "### 2. 量化介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24075712",
   "metadata": {},
   "source": [
    "量化即以较低的推理精度损失将连续取值（或者大量可能的离散取值）的浮点型模型权重或流经模型的张量数据定点近似（通常为INT8）为有限多个（或较少的）离散值的过程，它是以更少位数的数据类型用于近似表示32位有限范围浮点型数据的过程，而模型的输入输出依然是浮点型。这样的好处是可以减小模型尺寸大小，减少模型内存占用，加快模型推理速度，降低功耗等。\\\n",
    "如上所述，与FP32类型相比，FP16、INT8、INT4等低精度数据表达类型所占用空间更小。使用低精度数据表达类型替换高精度数据表达类型，可以大幅降低存储空间和传输时间。而低比特的计算性能也更高，INT8相对比FP32的加速比可达到3倍甚至更高，对于相同的计算，功耗上也有明显优势。\\\n",
    "当前业界量化方案主要分为两种：感知量化训练（Quantization Aware Training）和训练后量化（Post-training Quantization）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e9c5d",
   "metadata": {},
   "source": [
    "### 3. 实验环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d1b65",
   "metadata": {},
   "source": [
    "环境要求\n",
    "- ModelArts弹性云服务器，包含x86_64CPU和昇腾310推理芯片 \n",
    "- 操作系统：Ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e101d6",
   "metadata": {},
   "source": [
    "### 4. 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2abd56d",
   "metadata": {},
   "source": [
    "### 4.1 量化模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb364e-2ff1-4d6b-bb1a-ae5e9426f0c4",
   "metadata": {},
   "source": [
    "首先实现基本的量化公式，具体代码如下\n",
    "$$S = \\frac{r_{max}-r_{min}}{q_{max}-q_{min}} \\\\\n",
    "Z = round(q_{max} - \\frac{r_{max}}{S})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d433f4be-b632-4b95-b6c0-2d0f9423d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scale_zero_point(min_val, max_val, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2. ** num_bits - 1.\n",
    "    scale = float((max_val - min_val) / (qmax - qmin))  # S=(rmax-rmin)/(qmax-qmin)\n",
    "    zero_point = round(qmax - max_val / scale)  # Z=round(qmax-rmax/scale)\n",
    "\n",
    "    if zero_point < qmin:\n",
    "        zero_point = qmin\n",
    "    elif zero_point > qmax:\n",
    "        zero_point = qmax\n",
    "\n",
    "    return scale, zero_point\n",
    "\n",
    "\n",
    "def quantize_tensor(x, scale, zero_point, num_bits=8, signed=False):\n",
    "    # TODO 请根据公式实现张量的量化操作\n",
    "    if signed:\n",
    "        qmin = - 2. ** (num_bits - 1)\n",
    "        qmax = 2. ** (num_bits - 1) - 1\n",
    "    else:\n",
    "        qmin = 0.\n",
    "        qmax = 2. ** num_bits - 1.\n",
    "\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x = (q_x.clip(qmin, qmax)).round()  # q=round(r/S+Z)\n",
    "    return q_x.astype(ms.float32)       # 由于mindspore不支持int类型的运算，因此我们还是用float来表示整数\n",
    "\n",
    "def dequantize_tensor(q_x, scale, zero_point):\n",
    "    return scale * (q_x - zero_point)  # r=S(q-Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88bc10a-46fa-4bd4-94b0-f024768b85df",
   "metadata": {},
   "source": [
    "### 4.2 量化基类实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89e23f-c5c2-42da-9e8c-fd8437a7d83d",
   "metadata": {},
   "source": [
    "在量化过程中，需要先统计样本和中间层的最大最小值，同时也涉及到量化、反量化操作，因此将这些功能封装成一个QParam类\n",
    "\n",
    "接着定义基本的量化基类\n",
    "\n",
    "* `__init__`函数：指定量化的位数外，还需指定是否提供量化输入 (qi) 及输出参数 (qo)。在前面也提到，不是每一个网络模块都需要统计输入的 min、max，大部分中间层都是用上一层的 qo 来作为自己的 qi 的，另外有些中间层的激活函数也是直接用上一层的 qi 来作为自己的 qi 和 qo。\n",
    "* `freeze` 函数：这个函数会在统计完 min、max 后发挥作用。正如上文所说的，公式 (4) 中有很多项是可以提前计算好的，freeze 就是把这些项提前固定下来，同时也将网络的权重由浮点实数转化为定点整数。\n",
    "* `quantize_inference`函数：这个函数主要是量化 inference 的时候会使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5df4a4e-fb7a-4a7b-a637-0763a2fa0146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QParam(object):\n",
    "    def __init__(self, num_bits=8):\n",
    "        self.num_bits = num_bits\n",
    "        self.scale = None\n",
    "        self.zero_point = None\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "\n",
    "    def update(self, tensor):\n",
    "        # 用来统计 min、max \n",
    "        if self.max is None or self.max < tensor.max():\n",
    "            self.max = tensor.max()\n",
    "        self.max = 0 if self.max < 0 else self.max\n",
    "\n",
    "        if self.min is None or self.min > tensor.min():\n",
    "            self.min = tensor.min()\n",
    "        self.min = 0 if self.min > 0 else self.min\n",
    "\n",
    "        self.scale, self.zero_point = calculate_scale_zero_point(self.min, self.max, self.num_bits)\n",
    "\n",
    "    def quantize_tensor(self, tensor):\n",
    "        return quantize_tensor(tensor, self.scale, self.zero_point, num_bits=self.num_bits)\n",
    "\n",
    "    def dequantize_tensor(self, q_x):\n",
    "        return dequantize_tensor(q_x, self.scale, self.zero_point)\n",
    "    \n",
    "class QModule(nn.Cell):\n",
    "    def __init__(self, qi=True, qo=True, num_bits=8):\n",
    "        super(QModule, self).__init__()\n",
    "        if qi:\n",
    "            self.qi = QParam(num_bits=num_bits)\n",
    "        if qo:\n",
    "            self.qo = QParam(num_bits=num_bits)\n",
    "\n",
    "    def freeze(self):\n",
    "        pass\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        raise NotImplementedError('quantize_inference should be implemented.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1956f-fb2e-48ad-97ff-e003ba96188a",
   "metadata": {},
   "source": [
    "### 4.3 量化卷积模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd70af6-9df5-48b0-bbb5-abb089653275",
   "metadata": {},
   "source": [
    "量化卷积模块包括\n",
    "\n",
    "* `__init__` 函数：需要传入`conv_module` 模块，这个模块对应全精度的卷积层，另外的 `qw` 参数则是用来统计 weight 的 min、max 以及对 weight 进行量化用的。\n",
    "* `freeze`函数：这个函数主要就是计算公式中的 $M、q_w、q_b$，其中$M$应该由移位来实现定点化加速，为了实现方便，在此用原始的数学操作进行代替\n",
    "* `construct`函数：这个函数和正常的 construct一样，也是在 float 上进行的，只不过需要统计输入输出以及 weight 的 min、max 而已。其中这里需要对 weight 量化到 int8 然后又反量化回 float，这里其实就是所谓的伪量化节点，因为我们在实际量化 inference 的时候会把 weight 量化到 int8，这个过程本身是有精度损失的 (来自四舍五入的 round 带来的截断误差)，所以在统计 min、max 的时候，需要把这个过程带来的误差也模拟进去。\n",
    "* `quantize_inference` 函数：这个函数在实际 inference 的时候会被调用。注意，这个函数里面的卷积操作是在 int 上进行的，这是量化推理加速的关键「当然，由于 mindspore的限制，我们仍然是在 float 上计算，只不过数值都是整数。这也可以看出量化推理是跟底层实现紧密结合的技术」。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94735d-e991-4473-beb1-5729361b8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv2d(QModule):\n",
    "    def __init__(self, conv_module, qi=True, qo=True, num_bits=8):\n",
    "        super(QConv2d, self).__init__(qi=qi, qo=qo, num_bits=num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.conv_module = conv_module\n",
    "        self.qw = QParam(num_bits=num_bits)\n",
    "        self.M = None\n",
    "\n",
    "    def freeze(self, qi=None, qo=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "        if qo is not None:\n",
    "            self.qo = qo\n",
    "\n",
    "        self.M = self.qw.scale * self.qi.scale / self.qo.scale\n",
    "\n",
    "        self.conv_module.weight = self.qw.quantize_tensor(self.conv_module.weight)\n",
    "        self.conv_module.weight = self.conv_module.weight - self.qw.zero_point\n",
    "\n",
    "        self.conv_module.bias = quantize_tensor(self.conv_module.bias,\n",
    "                                                scale=self.qi.scale * self.qw.scale,\n",
    "                                                zero_point=0, num_bits=32, signed=True)\n",
    "\n",
    "    def construct(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = self.qi.quantize_tensor(x)\n",
    "            x = self.qi.dequantize_tensor(x)\n",
    "\n",
    "        self.qw.update(self.conv_module.weight)\n",
    "        self.conv_module.weight = self.qw.quantize_tensor(self.conv_module.weight)\n",
    "        self.conv_module.weight = self.qw.dequantize_tensor(self.conv_module.weight)\n",
    "        x = ops.conv2d(x, self.conv_module.weight, stride=self.conv_module.stride, pad_mode=self.conv_module.pad_mode)\n",
    "        if self.conv_module.bias is not None:\n",
    "            x = ops.bias_add(x, self.conv_module.bias)\n",
    "\n",
    "        if hasattr(self, 'qo'):\n",
    "            self.qo.update(x)\n",
    "            x = self.qo.quantize_tensor(x)\n",
    "            x = self.qo.dequantize_tensor(x)\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        x = self.M * x\n",
    "        x = x.round()\n",
    "        x = x + self.qo.zero_point\n",
    "        x = x.clip(0., 2. ** self.num_bits - 1.).round()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1723dd5-e53d-40e9-8160-967afb8f5c54",
   "metadata": {},
   "source": [
    "### 4.4 量化全连接层模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82740c5b-3ebe-48ab-807f-2001ae624240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDense(QModule):\n",
    "    def __init__(self, fc_module, qi=True, qo=True, num_bits=8):\n",
    "        super(QDense, self).__init__(qi=qi, qo=qo, num_bits=num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.fc_module = fc_module\n",
    "        self.qw = QParam(num_bits=num_bits)\n",
    "        self.M = ms.Tensor([])\n",
    "\n",
    "    def freeze(self, qi=None, qo=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "        if qo is not None:\n",
    "            self.qo = qo\n",
    "\n",
    "        self.M = self.qw.scale * self.qi.scale / self.qo.scale\n",
    "\n",
    "        self.fc_module.weight = self.qw.quantize_tensor(self.fc_module.weight)\n",
    "        self.fc_module.weight = self.fc_module.weight.data - self.qw.zero_point\n",
    "        self.fc_module.bias = quantize_tensor(self.fc_module.bias,\n",
    "                                              scale=self.qi.scale * self.qw.scale,\n",
    "                                              zero_point=0,\n",
    "                                              num_bits=32,\n",
    "                                              signed=True)\n",
    "\n",
    "    def construct(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = self.qi.quantize_tensor(x)\n",
    "            x = self.qi.dequantize_tensor(x)\n",
    "\n",
    "        self.qw.update(self.fc_module.weight)\n",
    "        self.fc_module.weight = self.qw.quantize_tensor(self.fc_module.weight)\n",
    "        self.fc_module.weight = self.qw.dequantize_tensor(self.fc_module.weight)\n",
    "        x = ops.matmul(x, self.fc_module.weight.T)\n",
    "        x = ops.bias_add(x, self.fc_module.bias)\n",
    "        if hasattr(self, 'qo'):\n",
    "            self.qo.update(x)\n",
    "            x = self.qo.quantize_tensor(x)\n",
    "            x = self.qo.dequantize_tensor(x)\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.fc_module(x)\n",
    "        x = self.M * x\n",
    "        x = x.round()\n",
    "        x = x + self.qo.zero_point\n",
    "        x = x.clip(0., 2. ** self.num_bits - 1.).round()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f1ff10-bb40-4360-9861-b7ca84709e92",
   "metadata": {},
   "source": [
    "### 4.5 量化ReLU模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7313f55-b53b-44cf-a2d1-84b570a02c7c",
   "metadata": {},
   "source": [
    "大体内容与量化卷积模块相似，其中需要注意，在`quantize_inference`函数中，量化零点非真实的0，需要特别注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935b5045-2a31-4e4d-a753-518702456d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QReLU(QModule):\n",
    "    def __init__(self, qi=False, num_bits=None):\n",
    "        super(QReLU, self).__init__(qi=qi, num_bits=num_bits)\n",
    "\n",
    "    def freeze(self, qi=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "\n",
    "    def construct(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = self.qi.quantize_tensor(x)\n",
    "            x = self.qi.dequantize_tensor(x)\n",
    "        x = ops.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x[x < self.qi.zero_point] = self.qi.zero_point\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a157dc-8bd3-4d7c-af95-f14eaebebdfc",
   "metadata": {},
   "source": [
    "### 4.6 量化最大池化模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85463720-1c36-4b4a-83c1-52ca5599ce14",
   "metadata": {},
   "source": [
    "大体内容与量化卷积模块相似，在量化推理时，因为最大池化原理就是取区域最大值作为输出，故直接进行算子运算即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c4472f-3bc6-4ec4-8443-a6f72556459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMaxPooling2d(QModule):\n",
    "    def __init__(self, max_pool_module, qi=False, num_bits=None):\n",
    "        super(QMaxPooling2d, self).__init__(qi=qi, num_bits=num_bits)\n",
    "        self.max_pool_module = max_pool_module\n",
    "\n",
    "    def freeze(self, qi=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "\n",
    "    def construct(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = self.qi.quantize_tensor(x)\n",
    "            x = self.qi.dequantize_tensor(x)\n",
    "        x = self.max_pool_module(x)\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        return self.max_pool_module(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ebc19-0920-4f3d-b422-e7ae9d90d1bc",
   "metadata": {},
   "source": [
    "### 4.7 量化VGG网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce154b-1449-4dbc-833f-86b4be49f9d1",
   "metadata": {},
   "source": [
    "量化卷积模块包括\n",
    "\n",
    "* `__init__`函数：基本的算子定义\n",
    "* `construct`函数：网络正向传播模块\n",
    "* `quantize`函数：量化网络模块\n",
    "* `quantize_forward`函数：量化正向传播模块\n",
    "* `freeze`函数：量化参数冻结模块\n",
    "* `quantize_inference`函数：量化推理模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cada95e-cd55-432b-bcab-b2a2906d8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg(nn.Cell):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(Vgg, self).__init__()\n",
    "        self.layer1_conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, has_bias=True)\n",
    "        self.layer1_relu1 = nn.ReLU()\n",
    "        self.layer1_conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, has_bias=True)\n",
    "        self.layer1_relu2 = nn.ReLU()\n",
    "        self.layer1_maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.layer2_conv1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, has_bias=True)\n",
    "        self.layer2_relu1 = nn.ReLU()\n",
    "        self.layer2_conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, has_bias=True)\n",
    "        self.layer2_relu2 = nn.ReLU()\n",
    "        self.layer2_maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.layer3_conv1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, has_bias=True)\n",
    "        self.layer3_relu1 = nn.ReLU()\n",
    "        self.layer3_conv2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, has_bias=True)\n",
    "        self.layer3_relu2 = nn.ReLU()\n",
    "        self.layer3_conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, has_bias=True)\n",
    "        self.layer3_relu3 = nn.ReLU()\n",
    "        self.layer3_maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.layer4_conv1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, has_bias=True)\n",
    "        self.layer4_relu1 = nn.ReLU()\n",
    "        self.layer4_conv2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, has_bias=True)\n",
    "        self.layer4_relu2 = nn.ReLU()\n",
    "        self.layer4_conv3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, has_bias=True)\n",
    "        self.layer4_relu3 = nn.ReLU()\n",
    "        self.layer4_maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.layer5_conv1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, has_bias=True)\n",
    "        self.layer5_relu1 = nn.ReLU()\n",
    "        self.layer5_conv2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, has_bias=True)\n",
    "        self.layer5_relu2 = nn.ReLU()\n",
    "        self.layer5_conv3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, has_bias=True)\n",
    "        self.layer5_relu3 = nn.ReLU()\n",
    "        self.layer5_maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fullyconnect1 = nn.Dense(512 * 7 * 7, 4096)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.fullyconnect2 = nn.Dense(4096, 4096)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.fullyconnect3 = nn.Dense(4096, num_classes)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.layer1_conv1(x)\n",
    "        x = self.layer1_relu1(x)\n",
    "        x = self.layer1_conv2(x)\n",
    "        x = self.layer1_relu2(x)\n",
    "        x = self.layer1_maxpool(x)\n",
    "\n",
    "        x = self.layer2_conv1(x)\n",
    "        x = self.layer2_relu1(x)\n",
    "        x = self.layer2_conv2(x)\n",
    "        x = self.layer2_relu2(x)\n",
    "        x = self.layer2_maxpool(x)\n",
    "\n",
    "        x = self.layer3_conv1(x)\n",
    "        x = self.layer3_relu1(x)\n",
    "        x = self.layer3_conv2(x)\n",
    "        x = self.layer3_relu2(x)\n",
    "        x = self.layer3_conv3(x)\n",
    "        x = self.layer3_relu3(x)\n",
    "        x = self.layer3_maxpool(x)\n",
    "\n",
    "        x = self.layer4_conv1(x)\n",
    "        x = self.layer4_relu1(x)\n",
    "        x = self.layer4_conv2(x)\n",
    "        x = self.layer4_relu2(x)\n",
    "        x = self.layer4_conv3(x)\n",
    "        x = self.layer4_relu3(x)\n",
    "        x = self.layer4_maxpool(x)\n",
    "\n",
    "        x = self.layer5_conv1(x)\n",
    "        x = self.layer5_relu1(x)\n",
    "        x = self.layer5_conv2(x)\n",
    "        x = self.layer5_relu2(x)\n",
    "        x = self.layer5_conv3(x)\n",
    "        x = self.layer5_relu3(x)\n",
    "        x = self.layer5_maxpool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fullyconnect1(x)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.fullyconnect2(x)\n",
    "        x = self.relu_2(x)\n",
    "        x = self.fullyconnect3(x)\n",
    "        return x\n",
    "\n",
    "    def quantize(self, num_bits=8):\n",
    "        # 第一个卷积模块需要获取量化输入，其余模块会复用之前的量化输出\n",
    "        self.qlayer1_conv1 = QConv2d(self.layer1_conv1, qi=True, qo=True, num_bits=num_bits)\n",
    "        self.qlayer1_relu1 = QReLU()\n",
    "        self.qlayer1_conv2 = QConv2d(self.layer1_conv2, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qlayer1_relu2 = QReLU()\n",
    "        self.qlayer1_maxpool2d = QMaxPooling2d(self.layer1_maxpool)\n",
    "\n",
    "        self.qlayer2_conv1 = QConv2d(self.layer2_conv1, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qlayer2_relu1 = QReLU()\n",
    "        self.qlayer2_conv2 = QConv2d(self.layer2_conv2, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qlayer2_relu2 = QReLU()\n",
    "        self.qlayer2_maxpool2d = QMaxPooling2d(self.layer2_maxpool)\n",
    "\n",
    "        self.qlayer3_conv1 = QConv2d(self.layer3_conv1, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qlayer3_relu1 = QReLU()\n",
    "        self.qlayer3_conv2 = QConv2d(self.layer3_conv2, qi=False, qo=True,num_bits=num_bits)\n",
    "        self.qlayer3_relu2 = QReLU()\n",
    "        self.qlayer3_conv3 = QConv2d(self.layer3_conv3, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qlayer3_relu3 = QReLU()\n",
    "        self.qlayer3_maxpool2d = QMaxPooling2d(self.layer3_maxpool)\n",
    "\n",
    "        self.qlayer4_conv1 = QConv2d(self.layer4_conv1, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qlayer4_relu1 = QReLU()\n",
    "        self.qlayer4_conv2 = QConv2d(self.layer4_conv2, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qlayer4_relu2 = QReLU()\n",
    "        self.qlayer4_conv3 = QConv2d(self.layer4_conv3, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qlayer4_relu3 = QReLU()\n",
    "        self.qlayer4_maxpool2d = QMaxPooling2d(self.layer4_maxpool)\n",
    "\n",
    "        self.qlayer5_conv1 = QConv2d(self.layer5_conv1, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qlayer5_relu1 = QReLU()\n",
    "        self.qlayer5_conv2 = QConv2d(self.layer5_conv2, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qlayer5_relu2 = QReLU()\n",
    "        self.qlayer5_conv3 = QConv2d(self.layer5_conv3, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qlayer5_relu3 = QReLU()\n",
    "        self.qlayer5_maxpool2d = QMaxPooling2d(self.layer5_maxpool)\n",
    "\n",
    "        self.qfc1 = QDense(self.fullyconnect1, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qfc1_relu = QReLU()\n",
    "        self.qfc2 = QDense(self.fullyconnect2, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qfc2_relu = QReLU()\n",
    "        self.qfc3 = QDense(self.fullyconnect3, qi=False, qo=True, num_bits=num_bits)\n",
    "\n",
    "    def quantize_forward(self, x):\n",
    "        x = self.qlayer1_conv1(x)\n",
    "        x = self.qlayer1_relu1(x)\n",
    "        x = self.qlayer1_conv2(x)\n",
    "        x = self.qlayer1_relu2(x)\n",
    "        x = self.qlayer1_maxpool2d(x)\n",
    "\n",
    "        x = self.qlayer2_conv1(x)\n",
    "        x = self.qlayer2_relu1(x)\n",
    "        x = self.qlayer2_conv2(x)\n",
    "        x = self.qlayer2_relu2(x)\n",
    "        x = self.qlayer2_maxpool2d(x)\n",
    "\n",
    "        x = self.qlayer3_conv1(x)\n",
    "        x = self.qlayer3_relu1(x)\n",
    "        x = self.qlayer3_conv2(x)\n",
    "        x = self.qlayer3_relu2(x)\n",
    "        x = self.qlayer3_conv3(x)\n",
    "        x = self.qlayer3_relu3(x)\n",
    "        x = self.qlayer3_maxpool2d(x)\n",
    "\n",
    "        x = self.qlayer4_conv1(x)\n",
    "        x = self.qlayer4_relu1(x)\n",
    "        x = self.qlayer4_conv2(x)\n",
    "        x = self.qlayer4_relu2(x)\n",
    "        x = self.qlayer4_conv3(x)\n",
    "        x = self.qlayer4_relu3(x)\n",
    "        x = self.qlayer4_maxpool2d(x)\n",
    "\n",
    "        x = self.qlayer5_conv1(x)\n",
    "        x = self.qlayer5_relu1(x)\n",
    "        x = self.qlayer5_conv2(x)\n",
    "        x = self.qlayer5_relu2(x)\n",
    "        x = self.qlayer5_conv3(x)\n",
    "        x = self.qlayer5_relu3(x)\n",
    "        x = self.qlayer5_maxpool2d(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.qfc1(x)\n",
    "        x = self.qfc1_relu(x)\n",
    "        x = self.qfc2(x)\n",
    "        x = self.qfc2_relu(x)\n",
    "        x = self.qfc3(x)\n",
    "        return x\n",
    "\n",
    "    def freeze(self):\n",
    "        # 冻结网络参数时，除第一个卷积模块不需要指定量化输入，其余模块都需要指定量化输入\n",
    "        self.qlayer1_conv1.freeze()\n",
    "        self.qlayer1_relu1.freeze(qi=self.qlayer1_conv1.qo)\n",
    "        self.qlayer1_conv2.freeze(qi=self.qlayer1_conv1.qo)\n",
    "        self.qlayer1_relu2.freeze(qi=self.qlayer1_conv2.qo)\n",
    "        self.qlayer1_maxpool2d.freeze(qi=self.qlayer1_conv2.qo)\n",
    "\n",
    "        self.qlayer2_conv1.freeze(qi=self.qlayer1_conv2.qo)\n",
    "        self.qlayer2_relu1.freeze(qi=self.qlayer2_conv1.qo)\n",
    "        self.qlayer2_conv2.freeze(qi=self.qlayer2_conv1.qo)\n",
    "        self.qlayer2_relu2.freeze(qi=self.qlayer2_conv2.qo)\n",
    "        self.qlayer2_maxpool2d.freeze(qi=self.qlayer2_conv2.qo)\n",
    "\n",
    "        self.qlayer3_conv1.freeze(qi=self.qlayer2_conv2.qo)\n",
    "        self.qlayer3_relu1.freeze(qi=self.qlayer3_conv1.qo)\n",
    "        self.qlayer3_conv2.freeze(qi=self.qlayer3_conv1.qo)\n",
    "        self.qlayer3_relu2.freeze(qi=self.qlayer3_conv2.qo)\n",
    "        self.qlayer3_conv3.freeze(qi=self.qlayer3_conv2.qo)\n",
    "        self.qlayer3_relu3.freeze(qi=self.qlayer3_conv3.qo)\n",
    "        self.qlayer3_maxpool2d.freeze(qi=self.qlayer3_conv3.qo)\n",
    "\n",
    "        self.qlayer4_conv1.freeze(qi=self.qlayer3_conv3.qo)\n",
    "        self.qlayer4_relu1.freeze(qi=self.qlayer4_conv1.qo)\n",
    "        self.qlayer4_conv2.freeze(qi=self.qlayer4_conv1.qo)\n",
    "        self.qlayer4_relu2.freeze(qi=self.qlayer4_conv2.qo)\n",
    "        self.qlayer4_conv3.freeze(qi=self.qlayer4_conv2.qo)\n",
    "        self.qlayer4_relu3.freeze(qi=self.qlayer4_conv3.qo)\n",
    "        self.qlayer4_maxpool2d.freeze(qi=self.qlayer4_conv3.qo)\n",
    "\n",
    "        self.qlayer5_conv1.freeze(qi=self.qlayer4_conv3.qo)\n",
    "        self.qlayer5_relu1.freeze(qi=self.qlayer5_conv1.qo)\n",
    "        self.qlayer5_conv2.freeze(qi=self.qlayer5_conv1.qo)\n",
    "        self.qlayer5_relu2.freeze(qi=self.qlayer5_conv2.qo)\n",
    "        self.qlayer5_conv3.freeze(qi=self.qlayer5_conv2.qo)\n",
    "        self.qlayer5_relu3.freeze(qi=self.qlayer5_conv3.qo)\n",
    "        self.qlayer5_maxpool2d.freeze(qi=self.qlayer5_conv3.qo)\n",
    "\n",
    "        self.qfc1.freeze(qi=self.qlayer5_conv3.qo)\n",
    "        self.qfc1_relu.freeze(qi=self.qfc1.qo)\n",
    "        self.qfc2.freeze(qi=self.qfc1.qo)\n",
    "        self.qfc2_relu.freeze(qi=self.qfc2.qo)\n",
    "        self.qfc3.freeze(qi=self.qfc2.qo)\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        # 对输入x进行量化\n",
    "        qx = self.qlayer1_conv1.qi.quantize_tensor(x)\n",
    "\n",
    "        qx = self.qlayer1_conv1.quantize_inference(qx)\n",
    "        qx = self.qlayer1_relu1.quantize_inference(qx)\n",
    "        qx = self.qlayer1_conv2.quantize_inference(qx)\n",
    "        qx = self.qlayer1_relu2.quantize_inference(qx)\n",
    "        qx = self.qlayer1_maxpool2d.quantize_inference(qx)\n",
    "\n",
    "        qx = self.qlayer2_conv1.quantize_inference(qx)\n",
    "        qx = self.qlayer2_relu1.quantize_inference(qx)\n",
    "        qx = self.qlayer2_conv2.quantize_inference(qx)\n",
    "        qx = self.qlayer2_relu2.quantize_inference(qx)\n",
    "        qx = self.qlayer2_maxpool2d.quantize_inference(qx)\n",
    "\n",
    "        qx = self.qlayer3_conv1.quantize_inference(qx)\n",
    "        qx = self.qlayer3_relu1.quantize_inference(qx)\n",
    "        qx = self.qlayer3_conv2.quantize_inference(qx)\n",
    "        qx = self.qlayer3_relu2.quantize_inference(qx)\n",
    "        qx = self.qlayer3_conv3.quantize_inference(qx)\n",
    "        qx = self.qlayer3_relu3.quantize_inference(qx)\n",
    "        qx = self.qlayer3_maxpool2d.quantize_inference(qx)\n",
    "\n",
    "        qx = self.qlayer4_conv1.quantize_inference(qx)\n",
    "        qx = self.qlayer4_relu1.quantize_inference(qx)\n",
    "        qx = self.qlayer4_conv2.quantize_inference(qx)\n",
    "        qx = self.qlayer4_relu2.quantize_inference(qx)\n",
    "        qx = self.qlayer4_conv3.quantize_inference(qx)\n",
    "        qx = self.qlayer4_relu3.quantize_inference(qx)\n",
    "        qx = self.qlayer4_maxpool2d.quantize_inference(qx)\n",
    "\n",
    "        qx = self.qlayer5_conv1.quantize_inference(qx)\n",
    "        qx = self.qlayer5_relu1.quantize_inference(qx)\n",
    "        qx = self.qlayer5_conv2.quantize_inference(qx)\n",
    "        qx = self.qlayer5_relu2.quantize_inference(qx)\n",
    "        qx = self.qlayer5_conv3.quantize_inference(qx)\n",
    "        qx = self.qlayer5_relu3.quantize_inference(qx)\n",
    "        qx = self.qlayer5_maxpool2d.quantize_inference(qx)\n",
    "\n",
    "        qx = self.flatten(qx)\n",
    "\n",
    "        qx = self.qfc1.quantize_inference(qx)\n",
    "        qx = self.qfc1_relu.quantize_inference(qx)\n",
    "        qx = self.qfc2.quantize_inference(qx)\n",
    "        qx = self.qfc2_relu.quantize_inference(qx)\n",
    "        qx = self.qfc3.quantize_inference(qx)\n",
    "\n",
    "        # 对输出qx进行反量化\n",
    "        out = self.qfc3.qo.dequantize_tensor(qx)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a7494-9167-42c7-ae2c-596d80abecec",
   "metadata": {},
   "source": [
    "## 9. 数据预处理操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b2d4cb5-5991-41f0-92d9-5b40df363857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, target_size):\n",
    "    h, w = image.shape[:2]\n",
    "    th, tw = target_size\n",
    "    # 获取等比缩放后的尺寸\n",
    "    scale = min(th / h, tw / w)\n",
    "    oh, ow = round(h * scale), round(w * scale)\n",
    "    # 缩放图片，opencv缩放传入尺寸为（宽，高），这里采用线性差值算法\n",
    "    image = cv2.resize(image, (ow, oh), interpolation=cv2.INTER_LINEAR).astype(np.uint8)\n",
    "    # 将剩余部分进行填充\n",
    "    new_image = np.ones((th, tw, 3), dtype=np.uint8) * 114\n",
    "    new_image[:oh, :ow, :] = image\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def process_image(img_path):\n",
    "    # 读取图片，opencv读图后格式是BGR格式，需要转为RGB格式\n",
    "    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # 将图片等比resize至(224x224)\n",
    "    image = resize_image(image, (224, 224))\n",
    "    image = np.array(image, dtype=np.float32)\n",
    "    # 将图片标准化\n",
    "    image -= [125.307, 122.961, 113.8575]\n",
    "    image /= [51.5865, 50.847, 51.255]\n",
    "    # (h,w,c) -> (c,h,w) -> (1,c,h,w)\n",
    "    image = image.transpose((2, 0, 1))[None]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e62bb4-e8dc-468b-80b1-38900eb50fe7",
   "metadata": {},
   "source": [
    "## 10. 定义推理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61f2a044-e352-45bb-b167-a7fb99480b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_quantize(model, dataset):\n",
    "    print('*'*50)\n",
    "    print('Start quantize')\n",
    "    for img_path, label in dataset:\n",
    "        print(\"Start inference: {}\".format(img_path))\n",
    "        ndarray = process_image(img_path)\n",
    "        tensor = ms.Tensor(ndarray, ms.float32)\n",
    "        net_out = model.quantize_forward(tensor)\n",
    "        prob = ops.Softmax()(net_out)\n",
    "        print('Predict probability: {}'.format(np.around(prob.asnumpy(), 4)))\n",
    "        predict_cls = (ops.Argmax()(prob)).asnumpy().item()\n",
    "        print('Inference result: {}\\n'.format(predict_cls == label))\n",
    "\n",
    "\n",
    "def full_inference(model, dataset):\n",
    "    print('*' * 50)\n",
    "    print('Start full inference')\n",
    "    for img_path, label in dataset:\n",
    "        print(\"Start inference: {}\".format(img_path))\n",
    "        ndarray = process_image(img_path)\n",
    "        tensor = ms.Tensor(ndarray, ms.float32)\n",
    "        net_out = model(tensor)\n",
    "        prob = ops.Softmax()(net_out)\n",
    "        print('Predict probability: {}'.format(np.around(prob.asnumpy(), 4)))\n",
    "        predict_cls = (ops.Argmax()(prob)).asnumpy().item()\n",
    "        print('Inference result: {}\\n'.format(predict_cls == label))\n",
    "\n",
    "\n",
    "def quantize_inference(model, dataset):\n",
    "    print('*' * 50)\n",
    "    print('Start quantize inference')\n",
    "    for img_path, label in dataset:\n",
    "        print(\"Start inference: {}\".format(img_path))\n",
    "        ndarray = process_image(img_path)\n",
    "        tensor = ms.Tensor(ndarray, ms.float32)\n",
    "        net_out = model.quantize_inference(tensor)\n",
    "        prob = ops.Softmax()(net_out)\n",
    "        print('Predict probability: {}'.format(np.around(prob.asnumpy(), 4)))\n",
    "        predict_cls = (ops.Argmax()(prob)).asnumpy().item()\n",
    "        print('Inference result: {}\\n'.format(predict_cls == label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9cd81b-7c6c-4139-abcf-4fd9d2e7dd3a",
   "metadata": {},
   "source": [
    "## 11. 实验运行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550957b5-3db6-4fbf-a559-8b48c24f962e",
   "metadata": {},
   "source": [
    "第一步：初始化VGG网络并加载权重系数\n",
    "\n",
    "第二步：构建对应推理数据\n",
    "\n",
    "第三步：首先进行正常的网络推理，获取模型输出\n",
    "\n",
    "第四步：构建量化模型，此实验为`int8`量化\n",
    "\n",
    "第五步：进行量化推理，这里涉及到对中间特征图统计最大最小值\n",
    "\n",
    "第六步：对网络量化参数进行固定\n",
    "\n",
    "第七步：进行量化推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "764f2350-e075-44a5-88bf-f394cf67a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化VGG网络并加载权重系数\n",
    "net = Vgg(num_classes=4)\n",
    "load_param_into_net(net, load_checkpoint('vgg.ckpt'), strict_load=True)\n",
    "net.set_train(False)\n",
    "\n",
    "# 构建对应推理数据\n",
    "dataset = [('./data/daisy_demo.jpg', 0),\n",
    "           ('./data/roses_demo.jpg', 1),\n",
    "           ('./data/sunflowers_demo.jpg', 2),\n",
    "           ('./data/tulips_demo.jpg', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be54efa1-ea9a-4d97-8e4f-d9d48422c46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Start full inference\n",
      "Start inference: ./data/daisy_demo.jpg\n",
      "Predict probability: [[0.9528 0.0262 0.009  0.0121]]\n",
      "Inference result: True\n",
      "\n",
      "Start inference: ./data/roses_demo.jpg\n",
      "Predict probability: [[0.0238 0.8661 0.0019 0.1082]]\n",
      "Inference result: True\n",
      "\n",
      "Start inference: ./data/sunflowers_demo.jpg\n",
      "Predict probability: [[0.001  0.0005 0.9975 0.0011]]\n",
      "Inference result: True\n",
      "\n",
      "Start inference: ./data/tulips_demo.jpg\n",
      "Predict probability: [[0.0003 0.0071 0.0004 0.9922]]\n",
      "Inference result: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 首先进行正常的网络推理，获取模型输出\n",
    "full_inference(net, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0f2d69d-8849-454f-a065-d39b9bda96e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建量化模型，此实验为int8量化\n",
    "net.quantize(num_bits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7951cd3-18d7-4371-920d-6df61e6194db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Start quantize\n",
      "Start inference: ./data/daisy_demo.jpg\n",
      "Predict probability: [[0.9524 0.0263 0.0083 0.013 ]]\n",
      "Inference result: True\n",
      "\n",
      "Start inference: ./data/roses_demo.jpg\n",
      "Predict probability: [[0.0208 0.8843 0.0017 0.0932]]\n",
      "Inference result: True\n",
      "\n",
      "Start inference: ./data/sunflowers_demo.jpg\n",
      "Predict probability: [[0.0007 0.0003 0.9983 0.0007]]\n",
      "Inference result: True\n",
      "\n",
      "Start inference: ./data/tulips_demo.jpg\n",
      "Predict probability: [[0.0002 0.0056 0.0004 0.9938]]\n",
      "Inference result: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 进行量化推理，这里涉及到对中间特征图统计最大最小值\n",
    "direct_quantize(net, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e2d91af-7f3c-490d-8cc6-60f5909af970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对网络量化参数进行固定\n",
    "net.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba2b3e20-078a-45e8-b154-f7731bd25f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Start quantize inference\n",
      "Start inference: ./data/daisy_demo.jpg\n",
      "Predict probability: [[0.9607 0.0217 0.0072 0.0105]]\n",
      "Inference result: True\n",
      "\n",
      "Start inference: ./data/roses_demo.jpg\n",
      "Predict probability: [[0.0187 0.8884 0.0016 0.0913]]\n",
      "Inference result: True\n",
      "\n",
      "Start inference: ./data/sunflowers_demo.jpg\n",
      "Predict probability: [[0.0008 0.0003 0.9983 0.0006]]\n",
      "Inference result: True\n",
      "\n",
      "Start inference: ./data/tulips_demo.jpg\n",
      "Predict probability: [[0.0002 0.0055 0.0003 0.994 ]]\n",
      "Inference result: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 进行量化推理\n",
    "quantize_inference(net, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5 (v3.7.5:5c02a39a0b, Oct 14 2019, 18:49:57) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
